# smilehelper

Some people have trouble reading facial expressions due to Autism or Asperger's.  This difficulty in reading facial expressions can lead to relational issues, which can then affect many different aspects of their lives.  Forming new relationships may be harder, and they may be misunderstood by their peers or their colleagues. At home, their family members may find it difficult to connect with them.

My idea is to utilize on-device machine learning to quickly read the emotions of another person.  There will be a live camera view, where the camera is pointed at another person’s face.  The emotions of that person is displayed via text on the screen in real time.  Augmented reality can be used to display the emotions in other ways.  For instance, for kids, an animated cartoon character can be displayed on the person’s shoulder.  Or, there can be environmental effects such as rain or snow, that may mirror the person’s emotions.

An extension of this idea is to help people train their brains to read emotions on their own and to mirror their own facial experiences to match that of the other person. For this, both the front-facing and back-facing cameras can be used.  On the screen, the user is prompted to match the emotions of the other person.  A single-player, gamified version of this app will allow the user to train on their own.

 I believe that such an app would be useful and will enrich the lives of those with this disability.


Please see the [pdf file](cover_letter.pdf) for more details! 